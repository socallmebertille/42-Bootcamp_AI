<div align="center" class="text-center">
  <h1>42-Bootcamp_AI</h1>
  
  <img alt="last-commit" src="https://img.shields.io/github/last-commit/socallmebertille/42-Bootcamp_AI?style=flat&amp;logo=git&amp;logoColor=white&amp;color=0080ff" class="inline-block mx-1" style="margin: 0px 2px;">
  <img alt="repo-top-language" src="https://img.shields.io/github/languages/top/socallmebertille/42-Bootcamp_AI?style=flat&amp;color=0080ff" class="inline-block mx-1" style="margin: 0px 2px;">
  <img alt="repo-language-count" src="https://img.shields.io/github/languages/count/socallmebertille/42-Bootcamp_AI?style=flat&amp;color=0080ff" class="inline-block mx-1" style="margin: 0px 2px;">
  <p><em>Built with the tools and technologies:</em></p>
  <img alt="Markdown" src="https://img.shields.io/badge/Markdown-000000.svg?style=flat&amp;logo=Markdown&amp;logoColor=white" class="inline-block mx-1" style="margin: 0px 2px;">
  <img alt="GNU%20Bash" src="https://img.shields.io/badge/GNU%20Bash-4EAA25.svg?style=flat&amp;logo=GNU-Bash&amp;logoColor=white" class="inline-block mx-1" style="margin: 0px 2px;">
  <img alt="Python" src="https://img.shields.io/badge/python-2496ED.svg?style=flat&amp;logo=python&amp;logoColor=white" class="inline-block mx-1" style="margin: 0px 2px;">
</div>

<h2>Table of Contents</h2>
<ul class="list-disc pl-4 my-0">
  <li class="my-0"><a href="#get-started">Get started</a></li>
  <ul class="list-disc pl-4 my-0">
    <li class="my-0"><a href="#instal-the-project">Install the project</a></li>
    <li class="my-0"><a href="#install-the-correct-environment">Install the correct environment</a></li>
  </ul>
  <li class="my-0"><a href="#theoretical-concepts">Theoretical concepts</a></li>
  <ul class="list-disc pl-4 my-0">
    <li class="my-0"><a href="#multivariate-linear-regression">Multivariate Linear Regression</a></li>
    <li class="my-0"><a href="#logistic-regression">Logistic Regression</a></li>
  </ul>
</ul>

<h2>Get started</h2>

#### Install the project

```bash
git clone https://github.com/socallmebertille/42-Bootcamp_AI.git 42-Bootcamp_AI
cd 42-Bootcamp_AI
```

#### Install the correct environment

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh # installation

source ~/.bashrc                                # restart home for bash
source ~/.zshrc                                 # restart for zsh

uv --version                                    # version check
uv python install 3.13                          # installation version python

uv venv                                         # creation
source .venv/bin/activate                       # activation

uv pip install numpy pandas jupyter ruff pytest # installation of dependancies

python your_script.py                           # use

deactivate                                      # desactivation
```

<h2>Theoretical concepts</h2>

### Multivariate Linear Regression

> **Objective:** Predict a target variable from multiple features using polynomial models and gradient descent.

---

#### ðŸŽ¯ Key Concepts

```
Univariate:   Å· = Î¸â‚€ + Î¸â‚Â·x
Multivariate: Å· = Î¸â‚€ + Î¸â‚Â·xâ‚ + Î¸â‚‚Â·xâ‚‚ + ... + Î¸â‚™Â·xâ‚™
Polynomial:   Å· = Î¸â‚€ + Î¸â‚Â·x + Î¸â‚‚Â·xÂ² + Î¸â‚ƒÂ·xÂ³ + ...
```

| Type | Number of Features | Example |
|------|-------------------|---------|
| **Univariate** | 1 | Price vs Time |
| **Multivariate** | n â‰¥ 2 | Price vs Weight, Distance, Time |
| **Polynomial** | Powers of a feature | x, xÂ², xÂ³, ... |

---

#### ðŸ“ Mathematical Formulas

**Hypothesis (Prediction)**
```
h(X) = X' Â· Î¸
where X' = [1, xâ‚, xâ‚‚, ..., xâ‚™]  (with column of ones)
      Î¸ = [Î¸â‚€, Î¸â‚, Î¸â‚‚, ..., Î¸â‚™]áµ€
```

**Cost Function (MSE)**
```
J(Î¸) = 1/m Â· Î£(Å·áµ¢ - yáµ¢)Â²
```

**Gradient**
```
âˆ‡J(Î¸) = 1/m Â· X'áµ€ Â· (X'Â·Î¸ - Y)
```

**Gradient Descent**
```
Î¸_new = Î¸_old - Î± Â· âˆ‡J(Î¸)
```

> **Î± (alpha)**: Learning rate  
> **m**: Number of examples  
> **X'**: Feature matrix with column of ones


#### ðŸ”„ Standard ML Pipeline

```mermaid
graph LR
    A[ðŸ“Š Raw Data] --> B[âœ‚ï¸ Split 80/20]
    B --> C[ðŸ”¢ Polynomial Features]
    C --> D[ðŸ“ Normalize X]
    D --> E[ðŸŽ¯ Train Gradient Descent]
    E --> F[ðŸ“ˆ Evaluate on Test]
    F --> G{Overfitting?}
    G -->|Yes| H[â¬‡ï¸ Reduce Complexity]
    G -->|No| I{Underfitting?}
    I -->|Yes| J[â¬†ï¸ Increase Complexity]
    I -->|No| K[âœ… Best Model]
```

---

#### âš–ï¸ Underfitting vs Overfitting

| Phenomenon | Train MSE | Test MSE | Cause | Solution |
|-----------|-----------|----------|-------|----------|
| **Underfitting** | ðŸ”´ High | ðŸ”´ High | Model too simple | â¬†ï¸ Increase complexity |
| **Sweet Spot** | ðŸŸ¢ Low | ðŸŸ¢ Low | Good balance | âœ… Keep it! |
| **Overfitting** | ðŸŸ¢ Very low | ðŸ”´ High | Model too complex | â¬‡ï¸ Reduce complexity |

**Visualization**

```
MSE
 â”‚
 â”‚  Test â”€â”€â”€â”€â”€â”€â•®
 â”‚            â”‚ â•²
 â”‚            â”‚  â•²  â† Overfitting zone
 â”‚   Train â”€â”€â”€â”¼â”€â”€â”€â•²___
 â”‚           â•±     â•²
 â”‚          â•±   âœ…  â•²
 â”‚         â•±         â•²___
 â”‚        â•±               â•²___
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Complexity
     Underfitting    Optimal   Overfitting
```

---

#### ðŸ› ï¸ Essential Techniques

1. **Polynomial Features**
```python
X = [xâ‚, xâ‚‚]
     â†“ degree=2
X_poly = [xâ‚, xâ‚Â², xâ‚‚, xâ‚‚Â², xâ‚Â·xâ‚‚]
```

2. **Normalization (Z-score)**
```python
X_norm = (X - Î¼) / Ïƒ

Î¼ = mean
Ïƒ = standard deviation
â†’ Result: Î¼=0, Ïƒ=1
```

> **Why?** Speeds up convergence, prevents overflow

3. **Train/Test Split**
```
ðŸ“Š Dataset (100%)
    â”œâ”€ 80% Train â†’ Training
    â””â”€ 20% Test  â†’ Evaluation (unseen data)
```

---

#### ðŸ“Š Evaluation Metrics

| Metric | Formula | Interpretation |
|----------|---------|----------------|
| **MSE** | `1/m Â· Î£(Å· - y)Â²` | Mean Squared Error |
| **RMSE** | `âˆšMSE` | Same unit as Y |
| **MAE** | `1/m Â· Î£|Å· - y|` | Mean Absolute Error |
| **RÂ²** | `1 - (SS_res/SS_tot)` | % variance explained (0-1) |

---

#### ðŸ’» Code Structure

```python
# 1. Load & Split
X, Y = load_data()
Xtrain, Xtest, Ytrain, Ytest = data_spliter(X, Y, 0.8)

# 2. Polynomial Features
X_poly = add_polynomial_features(Xtrain, degree=3)

# 3. Normalize
X_norm = zscore(X_poly)

# 4. Train
theta = np.ones((n_features + 1, 1))
lr = MyLR(theta, alpha=1e-3, max_iter=100000)
lr.fit_(X_norm, Ytrain)

# 5. Evaluate
y_pred = lr.predict_(X_test_norm)
mse = lr.mse(y_pred, Ytest)
```

> ðŸ’¡ **Tip:** Always normalize X (not Y), always split train/test, always compare train/test MSE to detect overfitting!

---

### Logistic Regression

> **Objective:** Predict a category/label/cass variable with a classification algorithm.

---

#### ðŸŽ¯ Key Concepts

